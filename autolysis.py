# /// script
# requires-python = ">=3.9"
# dependencies = [
#   "pandas",
#   "seaborn",
#   "matplotlib",
#   "numpy",
#   "scipy",
#   "openai",
#   "scikit-learn",
#   "requests",
#   "ipykernel", 
# ]
# ///

import os
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import argparse
import requests
import json
import openai  # Ensure this library is installed: pip install openai

# Function to perform data analysis: summary statistics, missing values, and correlation matrix
def perform_data_analysis(dataframe):
    print("Starting data analysis...")  # Debugging line
    
    # Compute summary statistics for numerical columns
    stats_summary = dataframe.describe()

    # Identify missing values in the dataset
    missing_data = dataframe.isnull().sum()

    # Extract numeric columns for correlation matrix computation
    numeric_data = dataframe.select_dtypes(include=[np.number])

    # Calculate correlation matrix for numeric columns
    correlation_matrix = numeric_data.corr() if not numeric_data.empty else pd.DataFrame()

    print("Data analysis completed.")  # Debugging line
    return stats_summary, missing_data, correlation_matrix


# Function to identify outliers using the Interquartile Range (IQR) method
def identify_outliers(dataframe):
    print("Identifying outliers...")  # Debugging line
    
    # Extract numeric columns
    numeric_data = dataframe.select_dtypes(include=[np.number])

    # Calculate IQR and identify outliers in numeric columns
    Q1 = numeric_data.quantile(0.25)
    Q3 = numeric_data.quantile(0.75)
    IQR = Q3 - Q1
    outliers_count = ((numeric_data < (Q1 - 1.5 * IQR)) | (numeric_data > (Q3 + 1.5 * IQR))).sum()

    print("Outlier identification completed.")  # Debugging line
    return outliers_count


# Function to create visualizations: correlation heatmap, outliers plot, and distribution plot
def create_visualizations(correlation_matrix, outliers_count, dataframe, output_directory):
    print("Creating visualizations...")  # Debugging line
    
    # Generate a heatmap for the correlation matrix
    plt.figure(figsize=(10, 8))
    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)
    plt.title('Correlation Matrix')
    heatmap_path = os.path.join(output_directory, 'correlation_matrix.png')
    plt.savefig(heatmap_path)
    plt.close()

    # Check and plot outliers if present
    if not outliers_count.empty and outliers_count.sum() > 0:
        plt.figure(figsize=(10, 6))
        outliers_count.plot(kind='bar', color='red')
        plt.title('Outliers Detection')
        plt.xlabel('Columns')
        plt.ylabel('Number of Outliers')
        outliers_path = os.path.join(output_directory, 'outliers.png')
        plt.savefig(outliers_path)
        plt.close()
    else:
        print("No outliers detected for visualization.")
        outliers_path = None  # No file created for outliers

    # Generate a distribution plot for the first numeric column
    numeric_columns = dataframe.select_dtypes(include=[np.number]).columns
    if len(numeric_columns) > 0:
        first_numeric_col = numeric_columns[0]  # Select the first numeric column
        plt.figure(figsize=(10, 6))
        sns.histplot(dataframe[first_numeric_col], kde=True, color='blue', bins=30)
        plt.title(f'Distribution')
        distribution_path = os.path.join(output_directory, f'distribution_.png')
        plt.savefig(distribution_path)
        plt.close()
    else:
        distribution_path = None  # No numeric columns to plot

    print("Visualizations created.")  # Debugging line
    return heatmap_path, outliers_path, distribution_path


# Function to generate README.md with analysis narrative and visualizations
def generate_readme(stats_summary, missing_data, correlation_matrix, outliers_count, output_directory):
    print("Generating README file...")  # Debugging line
    
    # Write analysis report to a markdown file
    readme_path = os.path.join(output_directory, 'README.md')
    try:
        with open(readme_path, 'w') as file:
            file.write("# Automated Data Analysis Report\n\n")
           
            # Introduction Section
            file.write("## Introduction\n")
            file.write("This is an automated analysis of the dataset, providing summary statistics, visualizations, and insights from the data.\n\n")

            # Summary Statistics Section
            file.write("## Summary Statistics\n")
            file.write("The summary statistics of the dataset are as follows:\n")
            file.write("\n| Statistic    | Value |\n")
            file.write("|--------------|-------|\n")

            # Write summary statistics for each column (mean, std, min, etc.)
            for column in stats_summary.columns:
                file.write(f"| {column} - Mean | {stats_summary.loc['mean', column]:.2f} |\n")
                file.write(f"| {column} - Std Dev | {stats_summary.loc['std', column]:.2f} |\n")
                file.write(f"| {column} - Min | {stats_summary.loc['min', column]:.2f} |\n")
                file.write(f"| {column} - 25th Percentile | {stats_summary.loc['25%', column]:.2f} |\n")
                file.write(f"| {column} - 50th Percentile (Median) | {stats_summary.loc['50%', column]:.2f} |\n")
                file.write(f"| {column} - 75th Percentile | {stats_summary.loc['75%', column]:.2f} |\n")
                file.write(f"| {column} - Max | {stats_summary.loc['max', column]:.2f} |\n")
                file.write("|--------------|-------|\n")
            
            file.write("\n")

            # Missing Values Section (Formatted as Table)
            file.write("## Missing Values\n")
            file.write("The following columns contain missing values, with their respective counts:\n")
            file.write("\n| Column       | Missing Values Count |\n")
            file.write("|--------------|----------------------|\n")
            for column, count in missing_data.items():
                file.write(f"| {column} | {count} |\n")
            file.write("\n")

            # Outliers Detection Section (Formatted as Table)
            file.write("## Outliers Detection\n")
            file.write("The following columns contain outliers detected using the IQR method (values beyond the typical range):\n")
            file.write("\n| Column       | Outlier Count |\n")
            file.write("|--------------|---------------|\n")
            for column, count in outliers_count.items():
                file.write(f"| {column} | {count} |\n")
            file.write("\n")

            # Correlation Matrix Section
            file.write("## Correlation Matrix\n")
            file.write("Below is the correlation matrix of numerical features, indicating relationships between different variables:\n\n")
            file.write("![Correlation Matrix](correlation_matrix.png)\n\n")

            # Outliers Visualization Section
            file.write("## Outliers Visualization\n")
            file.write("This chart visualizes the number of outliers detected in each column:\n\n")
            file.write("![Outliers](outliers.png)\n\n")

            # Distribution Plot Section
            file.write("## Distribution of Data\n")
            file.write("Below is the distribution plot of the first numerical column in the dataset:\n\n")
            file.write("![Distribution](distribution_.png)\n\n")

            # Conclusion Section
            file.write("## Conclusion\n")
            file.write("The analysis has provided insights into the dataset, including summary statistics, outlier detection, and correlations between key variables.\n")
            file.write("The generated visualizations and statistical insights can help in understanding the patterns and relationships in the data.\n\n")

            # Adding Story Section
            file.write("## Data Story\n")
           
        print(f"README file generated: {readme_path}")  # Debugging line
        return readme_path
    except Exception as error:
        print(f"Error writing to README.md: {error}")
        return None


# Function to generate a detailed story using the new OpenAI API through the proxy
def generate_story_with_llm(prompt, context):
    print("Generating story using LLM...")  # Debugging line
    try:
        # Retrieve the AIPROXY_TOKEN from the environment variable
        token = os.environ["AIPROXY_TOKEN"]

        # Define the custom API base URL for the proxy
        api_url = "https://aiproxy.sanand.workers.dev/openai/v1/chat/completions"

        # Construct the full prompt
        full_prompt = f"""
        Based on the following data analysis, please generate a creative and engaging story. The story should include multiple paragraphs, a clear structure with an introduction, body, and conclusion, and should feel like a well-rounded narrative.

        Context:
        {context}

        Data Analysis Prompt:
        {prompt}

        The story should be elaborate and cover the following:
        - An introduction to set the context.
        - A detailed body that expands on the data points and explores their significance.
        - A conclusion that wraps up the analysis and presents any potential outcomes or lessons.
        - Use transitions to connect ideas and keep the narrative flowing smoothly.
        - Format the story with clear paragraphs and structure.
        """

        # Prepare headers
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {token}"
        }

        # Prepare the body with the model and prompt
        data = {
            "model": "gpt-4o-mini",  # Specific model for proxy
            "messages": [
                {"role": "system", "content": "You are a helpful assistant."},
                {"role": "user", "content": full_prompt}
            ],
            "max_tokens": 1000,
            "temperature": 0.7
        }

        # Send the POST request to the proxy
        response = requests.post(api_url, headers=headers, data=json.dumps(data))

        # Check for successful response
        if response.status_code == 200:
            # Extract the story from the response
            story_content = response.json()['choices'][0]['message']['content'].strip()
            print("Story generated.")  # Debugging line
            return story_content
        else:
            print(f"Error with request: {response.status_code} - {response.text}")
            return "Failed to generate story."

    except Exception as error:
        print(f"Error: {error}")
        return "Failed to generate story."


# Main function that integrates all the steps
def main(csv_filepath):
    print("Initiating the analysis...")  # Debugging line

    # Set the API token as an environment variable
  
    # Attempt to read the CSV file with 'ISO-8859-1' encoding to handle special characters
    try:
        dataframe = pd.read_csv(csv_filepath, encoding='ISO-8859-1')
        print("Dataset loaded successfully!")  # Debugging line
    except UnicodeDecodeError as error:
        print(f"Error reading file: {error}")
        return

    stats_summary, missing_data, correlation_matrix = perform_data_analysis(dataframe)

    # Debugging print
    print("Summary Statistics:")
    print(stats_summary)

    outliers_count = identify_outliers(dataframe)

    # Debugging print
    print("Outliers detected:")
    print(outliers_count)

    output_directory = "."
    os.makedirs(output_directory, exist_ok=True)

    # Visualize the data and check output paths
    heatmap_path, outliers_path, distribution_path = create_visualizations(correlation_matrix, outliers_count, dataframe, output_directory)

    print("Visualizations saved.")

    # Generate the story using the LLM
    story_content = generate_story_with_llm("Generate a nice and creative story from the analysis", 
                         context=f"Dataset Analysis:\nSummary Statistics:\n{stats_summary}\n\nMissing Values:\n{missing_data}\n\nCorrelation Matrix:\n{correlation_matrix}\n\nOutliers:\n{outliers_count}")

    # Create the README file with the analysis and the story
    readme_path = generate_readme(stats_summary, missing_data, correlation_matrix, outliers_count, output_directory)
    if readme_path:
        try:
            # Append the story to the README.md file
            with open(readme_path, 'a') as file:
                file.write("## Story\n")
                file.write(f"{story_content}\n")

            print(f"Analysis complete! Results saved in '{output_directory}' directory.")
            print(f"README file: {readme_path}")
            print(f"Visualizations: {heatmap_path}, {outliers_path}, {distribution_path}")
        except Exception as error:
            print(f"Error appending story to README.md: {error}")
    else:
        print("Error generating the README.md file.")

if __name__ == "__main__":
    import sys
    if len(sys.argv) < 2:
        print("Usage: uv run autolysis.py <dataset_path>")
        sys.exit(1)
    main(sys.argv[1])
